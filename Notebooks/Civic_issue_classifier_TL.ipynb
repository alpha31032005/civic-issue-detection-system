{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41fbb7e9-a515-41bb-82e6-f87d8dd6ac3b",
   "metadata": {},
   "source": [
    "**BUILDING AN IMAGE CLASSIFIER WHICH CLASSIFIES IMAGES INTO SEVERAL CIVIC CATEGORIES LIKE GARBAGE, POTHOLES, ETC.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0028ef75-cf44-4aaa-b2c0-78c63696c3c4",
   "metadata": {},
   "source": [
    "**IMPORTING REQUIREMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b04bc8ee-88a6-4ece-8f52-44a0c2399c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d1dff5b-1a70-4746-abc3-7c4d6812ef55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPU available: []\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeca46b-35a7-44e6-a263-201e17826ce6",
   "metadata": {},
   "source": [
    "**CALLING DATA FROM FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43427b3-7933-4c1e-86b3-029c6778b6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24571 files belonging to 4 classes.\n",
      "Found 3374 files belonging to 4 classes.\n",
      "Found 2666 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"E:\\\\My Programs\\\\civic_issue_classification\\\\dataset\\\\train\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"E:\\\\My Programs\\\\civic_issue_classification\\\\dataset\\\\val\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"E:\\\\My Programs\\\\civic_issue_classification\\\\dataset\\\\test\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69bbaa6b-8d10-4257-ba67-b97c365fa59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['electricpoles', 'fallentrees', 'garbage', 'pothole']\n"
     ]
    }
   ],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4094d6-878b-4ee3-a782-e72784e00a9f",
   "metadata": {},
   "source": [
    "**NORMALIZATION AND AUGMENTATION OF DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5397abc-e074-43b7-bbbc-9f59385dd1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General augmentation for all classes\n",
    "general_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.1),\n",
    "])\n",
    "\n",
    "# Stronger augmentation dedicated to garbage class\n",
    "# garbage_augmentation = tf.keras.Sequential([\n",
    "#     layers.RandomFlip(\"horizontal\"),\n",
    "#     layers.RandomRotation(0.2),\n",
    "#     layers.RandomZoom(0.2),\n",
    "#     layers.RandomContrast(0.2),\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e0a6e4f-468c-4b5f-98de-9ed2bc63ac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "train_ds = (\n",
    "    train_ds\n",
    "    .map(lambda x, y: (general_augmentation(x, training=True), y))\n",
    "    .map(lambda x, y: (normalization_layer(x), y))\n",
    "    .shuffle(1000)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    val_ds\n",
    "    .map(lambda x, y: (normalization_layer(x), y))\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_ds = (\n",
    "    test_ds\n",
    "    .map(lambda x, y: (normalization_layer(x), y))\n",
    "    .cache()\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ea1db-3181-4e36-a3c1-821d6158dd05",
   "metadata": {},
   "source": [
    "**TRIED AUGMENTING(OVERSAMPLING) GARBAGE CLASS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78490a34-db96-411e-b415-87de16bf70cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# garbage_ds = train_ds_raw.unbatch().filter(\n",
    "#     lambda x, y: tf.equal(tf.argmax(y), garbage_index)\n",
    "# )\n",
    "\n",
    "# garbage_ds = garbage_ds.batch(batch_size)\n",
    "# garbage_ds = garbage_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "# garbage_ds = garbage_ds.map(lambda x, y: (garbage_augmentation(x, training=True), y))\n",
    "\n",
    "# # Merge augmented garbage back\n",
    "# train_ds = train_ds.concatenate(garbage_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444c3e7-8ef7-49e3-8690-e3be03ff1eaa",
   "metadata": {},
   "source": [
    "**BUILDING MODEL AND USING TRANSFER LEARNING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75bb8d-867f-4392-ba55-f17ab3bf1411",
   "metadata": {},
   "source": [
    "**1. CUSTOM CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a60d2d43-d639-469d-b70c-956c32c743f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_civic_cnn(input_shape=(224,224,3), num_classes=4):\n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "#     model = models.Sequential([\n",
    "#         layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
    "#         layers.MaxPooling2D((2,2)),\n",
    "\n",
    "#         layers.Conv2D(64, (3,3), activation='relu'),\n",
    "#         layers.MaxPooling2D((2,2)),\n",
    "\n",
    "#         layers.Conv2D(128, (3,3), activation='relu'),\n",
    "#         layers.MaxPooling2D((2,2)),\n",
    "\n",
    "#         layers.Conv2D(256, (3,3), activation='relu'),\n",
    "#         layers.MaxPooling2D((2,2)),\n",
    "\n",
    "#         #layers.Flatten(),\n",
    "#         layers.GlobalAveragePooling2D(),\n",
    "#         layers.Dense(128, activation='relu'),\n",
    "#         layers.Dropout(0.5),\n",
    "#         layers.Dense(num_classes, activation='softmax')\n",
    "#     ])\n",
    "#     model.compile(optimizer='adam',\n",
    "#                   loss='categorical_crossentropy',\n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# model = build_civic_cnn()\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83052fd-6be6-4d1a-b887-464e91343e75",
   "metadata": {},
   "source": [
    "**ADDING CLASS WEIGHTS AND MENTIONING CHECKPOINTS AND EARLYSTOP**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b417311-044d-450e-8883-c173d87aa9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24571 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# RAW training dataset (ONLY for class weights)\n",
    "train_ds_raw = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"E:\\\\My Programs\\\\civic_issue_classification\\\\dataset\\\\train\",\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode=\"categorical\",\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2285163-7f83-4afd-b78e-64230d085154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {np.int64(0): np.float64(0.8448287718333104), np.int64(1): np.float64(0.7226764705882353), np.int64(2): np.float64(1.9606607085860197), np.int64(3): np.float64(1.083950944062114)}\n"
     ]
    }
   ],
   "source": [
    "#Adding class weights so that the balance is maintained between classes\n",
    "# ===============================\n",
    "# CLASS WEIGHTS\n",
    "# ===============================\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "labels = []\n",
    "\n",
    "assert train_ds_raw is not None, \"train_ds_raw must be defined before computing class weights\"\n",
    "for _, y in train_ds_raw.unbatch():\n",
    "    if len(y.shape) > 0:          # one-hot\n",
    "        labels.append(int(np.argmax(y.numpy())))\n",
    "    else:                         # integer labels\n",
    "        labels.append(int(y.numpy()))\n",
    "\n",
    "unique_classes = np.arange(len(class_names))\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=unique_classes,\n",
    "    y=labels\n",
    ")\n",
    "\n",
    "class_weights = dict(zip(unique_classes, class_weights))\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "#*******************************************************************************\n",
    "#Calculating steps per epoch\n",
    "# train_steps = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "# steps_per_epoch = train_steps\n",
    "# print(\"Steps per epoch:\", steps_per_epoch)\n",
    "#*******************************************************************************\n",
    "\n",
    "# ===============================\n",
    "# CALLBACKS\n",
    "# ===============================\n",
    "#Checkpoint for saving the best model only \n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "\n",
    "#Earlystop to avoid over computation\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=2,            # stop if val_loss doesn’t improve for 2 epochs\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988bf75e-7b76-4d0f-be98-85fa1a0823ca",
   "metadata": {},
   "source": [
    "**1.1 FITTING MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a433016-d535-4455-a3d5-e2cab50f1470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=val_ds,\n",
    "#     epochs=8,\n",
    "#     class_weight=class_weights,\n",
    "#     steps_per_epoch = steps_per_epoch,\n",
    "#     callbacks=[early_stop, checkpoint]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "587e1da3-d2d7-47a3-ab46-daaf63e87e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"civic_issue_model_customCNN.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1f8f8-a7b8-40f9-bcce-2b83200eb707",
   "metadata": {},
   "source": [
    "**1.2 PLOTTING THE LEARNINGS MADE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a53946a9-9623-40a5-b6e7-1caf04014aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history['accuracy'], label='train acc')\n",
    "# plt.plot(history.history['val_accuracy'], label='val acc')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(history.history['loss'], label='train loss')\n",
    "# plt.plot(history.history['val_loss'], label='val loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1f70e9-7c3d-42a7-977b-1d2629ce3574",
   "metadata": {},
   "source": [
    "**1.3 TESTING MODEL PERFORMANCE ON TEST_DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baeda934-98e3-4d95-b761-b0690b90d6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss, test_acc = model.evaluate(test_ds)   #testing model performance on the test data set\n",
    "# print(\"Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a729b69-70fd-4c8a-9689-b288ff57a972",
   "metadata": {},
   "source": [
    "**1.4 LET's TRY A DEMO IMAGE AND SEE HOW MODEL CLASSIFIES IT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20d692e5-f900-46ff-8b40-f7bb41fd849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# class_names = train_ds.class_names\n",
    "# print(\"Class names:\", class_names)\n",
    "\n",
    "# img_path = \"E:\\\\My Programs\\\\civic_issue_classification\\\\dataset\\\\train\\\\garbage\\\\1__aluminum-tin-cans-2_jpg.rf.3357c35942c2fcc9ec3ff705e89ed967.jpg\"  # replace with any test image\n",
    "# img = image.load_img(img_path, target_size=img_size)\n",
    "# img_array = image.img_to_array(img)/255.0\n",
    "# img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# pred = model.predict(img_array)\n",
    "# print(\"Predicted category:\", class_names[np.argmax(pred)])\n",
    "# print(\"Prediction probabilities:\", pred[0])  # show all 4 class probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "433da904-ee7f-4582-9c5b-30998a1770b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electricpoles 7271\n",
      "fallentrees 8500\n",
      "garbage 3133\n",
      "pothole 5667\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for cls in class_names:\n",
    "    print(cls, len(os.listdir(f\"E:\\\\My Programs\\\\civic_issue_classification\\\\dataset\\\\train\\\\{cls}\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd54a8b-9c50-4cdc-8cc3-f16465436b48",
   "metadata": {},
   "source": [
    "**1.5 PLOTTING CONFUSION MATRIX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87543954-2ae2-439d-8974-47ffb197e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# y_true, y_pred = [], []\n",
    "# for images, labels in test_ds:\n",
    "#     preds = model.predict(images)\n",
    "#     y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "#     y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "# cm = confusion_matrix(y_true, y_pred)\n",
    "# sns.heatmap(cm, annot=True, xticklabels=class_names, yticklabels=class_names, fmt='d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4415c-1288-4056-b71a-1743d684c467",
   "metadata": {},
   "source": [
    "**2. TRANSFER LEARNING MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaed609-c2c3-4f0f-b3b1-69f746bc56c7",
   "metadata": {},
   "source": [
    "**2.1 LOAD PRETRAINED MobileNetV2 (FEATURE EXTRACTOR)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b189de07-db34-4d68-a450-f4fc3f671192",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m base_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mapplications\u001b[38;5;241m.\u001b[39mMobileNetV2(\n\u001b[0;32m      2\u001b[0m     input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m      3\u001b[0m     include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Freeze pretrained weights\u001b[39;00m\n\u001b[0;32m      8\u001b[0m base_model\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")\n",
    "\n",
    "# Freeze pretrained weights\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ab64b-5c60-4f58-8d04-15bfd79c3884",
   "metadata": {},
   "source": [
    "**2.2 BUILDING TL MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fad6384-f671-49af-bf56-ee53e1b80a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(class_names), activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94728df2-a29b-4474-bbf2-4ac393b3ceb2",
   "metadata": {},
   "source": [
    "**2.3 COMPILING THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0e12946-df2e-4907-b54f-3dc4fbeae98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │         \u001b[38;5;34m5,120\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m516\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,427,588</span> (9.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,427,588\u001b[0m (9.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">167,044</span> (652.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m167,044\u001b[0m (652.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,260,544</span> (8.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,260,544\u001b[0m (8.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.Precision(name=\"precision\"),\n",
    "        tf.keras.metrics.Recall(name=\"recall\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86222977-8578-493a-80db-cc9e087adf81",
   "metadata": {},
   "source": [
    "**2.4 TRAINING THE MODEL (MODEL FIT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a58391-f199-45bb-a525-f766624c2035",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m      2\u001b[0m     train_ds,\n\u001b[0;32m      3\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mval_ds,\n\u001b[0;32m      4\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      5\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39mclass_weights,\n\u001b[0;32m      6\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stop, checkpoint]\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stop, checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb72c655-8d0d-4060-a42d-715cc9415378",
   "metadata": {},
   "source": [
    "**2.5 TEST SET EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13419461-67d2-4a62-8f7c-b4bdf882ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d563d7cf-8929-4149-bfde-b96578cfa651",
   "metadata": {},
   "source": [
    "**2.6 REPRESENTING CONFUSION MATRIX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb1691a-cc3e-433c-9985-8b69546d701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "for x, y in test_ds:\n",
    "    preds = model.predict(x, verbose=0)\n",
    "    y_true.extend(np.argmax(y.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\",\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix - MobileNetV2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910bb888-3734-47e7-baea-a334042f92e5",
   "metadata": {},
   "source": [
    "**2.7 SAVE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583e521-8ab5-4764-9c53-e26beb2ef5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# SAVE BASELINE MOBILE NET MODEL\n",
    "# =====================================================\n",
    "\n",
    "model.save(\"mobilenetv2_baseline.keras\")\n",
    "\n",
    "print(\"Baseline MobileNetV2 model saved successfully!\")\n",
    "\n",
    "assert os.path.exists(\"mobilenetv2_baseline.keras\"), \"Model save failed!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b87d952-06a1-49b8-b3f6-7ce7553112a0",
   "metadata": {},
   "source": [
    "**2.8 FINE-TUNE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac7db3-ff1e-4c9d-848d-85edc3ef7a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# FINE-TUNING (ADVANCED)\n",
    "# =====================================================\n",
    "\n",
    "# Unfreeze last 20 layers of MobileNetV2\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        tf.keras.metrics.Precision(name=\"precision\"),\n",
    "        tf.keras.metrics.Recall(name=\"recall\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "history_finetune = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stop, checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5f996-edc3-424b-9e1c-1b980c06125e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
